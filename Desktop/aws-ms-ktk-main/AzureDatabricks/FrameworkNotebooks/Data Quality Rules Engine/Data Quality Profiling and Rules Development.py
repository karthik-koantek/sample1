# Databricks notebook source
# MAGIC %md
# MAGIC # Data Quality Profiling and Rules Development

# COMMAND ----------

# MAGIC %md
# MAGIC #### Initialize

# COMMAND ----------

import ktk
from ktk import utilities as u
import datetime, json

# COMMAND ----------

dbutils.widgets.text(name="stepLogGuid", defaultValue="00000000-0000-0000-0000-000000000000", label="stepLogGuid")
dbutils.widgets.text(name="stepKey", defaultValue="-1", label="stepKey")
dbutils.widgets.text(name="tableName", defaultValue="", label="Table Name")
dbutils.widgets.text(name="deltaHistoryMinutes", defaultValue="-1", label="History Minutes")
dbutils.widgets.dropdown(name="profile", defaultValue="False", choices=["True", "False"], label="Profile")
dbutils.widgets.dropdown(name="useGEProfilingExpectations", defaultValue="False", choices=["True", "False"], label="Use Profiling Expectations")

widgets = ["stepLogGuid","stepKey","tableName","deltaHistoryMinutes","profile","useGEProfilingExpectations"]
secrets = []

# COMMAND ----------

snb = ktk.DataQualityRulesNotebook(widgets, secrets)

# COMMAND ----------

if snb.profile == "True":
  snb.profile = True
else:
  snb.profile = False
if snb.useGEProfilingExpectations == "True":
  snb.useGEProfilingExpectations = True
else:
  snb.useGEProfilingExpectations = False
p = {}
parameters = json.dumps(snb.mergeAttributes(p))
snb.log_notebook_start(parameters)
print("Parameters:")
snb.displayAttributes()

# COMMAND ----------

# MAGIC %md
# MAGIC #### Create Data Quality Rules Schema

# COMMAND ----------

snb.createDataQualityRulesSchema()

# COMMAND ----------

# MAGIC %md
# MAGIC #### Refresh Table

# COMMAND ----------

refreshed = u.refreshTable(snb.tableName)
if refreshed == False:
  snb.log_notebook_end(0)
  dbutils.notebook.exit("Table does not exist")
dfList = []
dfList.append(spark.table(snb.tableName))

# COMMAND ----------

# MAGIC %md
# MAGIC #### Obtain Change Delta

# COMMAND ----------

if snb.deltaHistoryMinutes != -1:
  try:
    print("Obtaining time travel delta")
    dfList.append(u.getTableChangeDelta(dfList[-1], snb.tableName, snb.deltaHistoryMinutes))
    if dfList[-1].count == 0:
      snb.log_notebook_end(0)
      dbutils.notebook.exit("No new or modified rows to process.")
  except Exception as e:
    err = {
      "sourceName" : "Data Quality Profiling and Rules Development: Obtain Change Delta",
      "errorCode" : "100",
      "errorDescription" : e.__class__.__name__
    }
    error = json.dumps(err)
    snb.log_notebook_error(error)
    raise(e)

snb.setGreatExpectationsDataset(dfList[-1])
dfGE = snb.dfGE

# COMMAND ----------

# MAGIC %md
# MAGIC #### Get Existing Expectations

# COMMAND ----------

try:
  expectation_list = snb.getDataQualityRulesForTable()
  for e in expectation_list:
    print(e)
  if len(expectation_list) == 0:
    print("No rules exist for this table, setting profile = True")
    profile = True
except Exception as e:
  err = {
    "sourceName" : "Data Quality Profiling and Rules Development: Get Existing Expectations",
    "errorCode" : "200",
    "errorDescription" : e.__class__.__name__
  }
  error = json.dumps(err)
  snb.log_notebook_error(error)
  raise(e)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Profile

# COMMAND ----------

if snb.profile == True:
  try:
    expectation_suite, validation_results, html = snb.profileGreatExpectations()
    displayHTML(snb.profiling_results_html)
    expectation_list = snb.formatGreatExpectationsList()
  except Exception as e:
    err = {
      "sourceName" : "Data Quality Profiling and Rules Development: Profile",
      "errorCode" : "300",
      "errorDescription" : e.__class__.__name__
    }
    error = json.dumps(err)
    snb.log_notebook_error(error)
    raise(e)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Edit Expectations
# MAGIC 
# MAGIC * The minimum expecations are table row count and column list.  These will be generated by default.
# MAGIC * You can start with the GE profiling expectation list as a baseline by setting "useGEProfilingExpectations" parameter to True.
# MAGIC * To edit/modify existing expectations, run the cell below and copy the results into a new cell. Make your edits, then run the cell.
# MAGIC * To add new expectations, type **dfGE.** and the TAB key to see autocompletion of all available expectation methods.

# COMMAND ----------

if snb.useGEProfilingExpectations == True:  
  for ex in snb.expectation_list:
    print("dfGE.{0}".format(ex))
    try:
      exec("dfGE.{0}".format(ex))
    except Exception as e:
      print("unable to exec expectation {0}".format(ex))

# COMMAND ----------

sql = "DESCRIBE {0}".format(snb.tableName)
display(spark.sql(sql))

# COMMAND ----------

sql = "SELECT * FROM {0}".format(snb.tableName)
display(spark.sql(sql))

# COMMAND ----------

#paste results from above cell here to edit and run them


# COMMAND ----------

#mandatory default tests
column_list=dfGE.get_table_columns()
row_count = dfGE.get_row_count()
dfGE.expect_table_row_count_to_be_between(min_value=0,max_value=row_count)
dfGE.expect_table_columns_to_match_ordered_list(column_list=column_list)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Validate Expectations

# COMMAND ----------

validation_results, html = snb.validateGreatExpectations(dfGE)
displayHTML(snb.validation_results_html)

# COMMAND ----------

failures = [x for x in snb.validation_results.results if not x.success]
if len(failures) > 0:
  print(failures)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Review Expectations

# COMMAND ----------

snb.expectation_suite = dfGE.get_expectation_suite(discard_failed_expectations=True)
expectation_list = snb.formatGreatExpectationsList()
expectation_list

# COMMAND ----------

validation_results, html = snb.validateGreatExpectations(dfGE)

# COMMAND ----------

failures = [x for x in snb.validation_results.results if not x.success]
if len(failures) > 0:
  print(failures)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Save Expectations

# COMMAND ----------

try:
  snb.saveExpectationList()
except Exception as e:
  err = {
    "sourceName" : "Data Quality Profiling and Rules Development: Save Expectations",
    "errorCode" : "400",
    "errorDescription" : e.__class__.__name__
  }
  error = json.dumps(err)
  snb.log_notebook_error(error)
  raise(e)

# COMMAND ----------

display(
spark.sql("""
SELECT * FROM goldprotected.DataQualityRule WHERE fullyQualifiedTableName = '{0}' ORDER BY effectiveStartDate DESC
""".format(snb.tableName))
)

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT fullyQualifiedTableName, size(rules) AS ExpectationCount
# MAGIC FROM goldprotected.DataQualityRule 
# MAGIC WHERE isActive = 1
# MAGIC ORDER BY fullyQualifiedTableName

# COMMAND ----------


